import fitz
import json
from pathlib import Path
from typing import Dict, List
from PIL import Image
from common_parameter import PDF_PATH, OUTPUT_DIR, TABLE_DPI
from logger import setup_advanced_logger # error ì‹œ Archive/logger.py ì‚¬ìš©í•  ê²ƒ 
import logging

logger = setup_advanced_logger(name="step3_image_generator", log_dir=OUTPUT_DIR, log_level=logging.INFO)


class TableImageGenerator:
    """í…Œì´ë¸”/ê·¸ë¦¼ ì´ë¯¸ì§€ ìƒì„±ê¸°"""
    
    def __init__(self, pdf_path: str, section_data_dir: str = "output/section_data"):
        """
        Args:
            pdf_path: PDF íŒŒì¼ ê²½ë¡œ
            section_data_dir: ì„¹ì…˜ ë°ì´í„° JSON ë””ë ‰í† ë¦¬
        """
        self.pdf_path = Path(pdf_path)
        self.doc = fitz.open(str(pdf_path))
        self.section_data_dir = Path(section_data_dir)
        
    def generate_table_image(self, page_num: int, bbox: List[float], 
                            output_path: Path, 
                            margin_top: int = 2, margin_bottom: int = 5, 
                            margin_left: int = 2, margin_right: int = 2, 
                            dpi: int = 120):
        """
        í…Œì´ë¸” ì´ë¯¸ì§€ ìƒì„±
        
        Args:
            page_num: í˜ì´ì§€ ë²ˆí˜¸ (1-based)
            bbox: [x0, y0, x1, y1]
            output_path: ì¶œë ¥ íŒŒì¼ ê²½ë¡œ
            margin_*: ê° ë°©í–¥ë³„ ì—¬ë°± (í”½ì…€)
            dpi: ì´ë¯¸ì§€ í•´ìƒë„ (DPI), ê¸°ë³¸ê°’ 120
        """
        page = self.doc[page_num - 1]
        
        # The BBox in JSON comes from Step 1 (DeepSeek), which typically uses a 1000x1000 normalized coordinate system.
        # PyMuPDF expects coordinates in PDF points (1/72 inch).
        # We must scale the 1000-based coordinates to the actual page dimensions in points.
        
        page_width = page.rect.width
        page_height = page.rect.height
        
        scale_x = page_width / 1000.0
        scale_y = page_height / 1000.0
        
        pdf_bbox = [
            bbox[0] * scale_x,
            bbox[1] * scale_y,
            bbox[2] * scale_x,
            bbox[3] * scale_y
        ]
        
        rect = fitz.Rect(pdf_bbox)
        
        # ìƒë‹¨(y0) ì¡°ì ˆ ë¡œì§
        rect.x0 = max(0, rect.x0 - margin_left)
        rect.y0 = max(0, rect.y0 - margin_top)
        rect.x1 = min(page.rect.width, rect.x1 + margin_right)
        rect.y1 = min(page.rect.height, rect.y1 + margin_bottom)
        
        # ìœ íš¨ì„± ê²€ì‚¬
        if rect.width <= 0 or rect.height <= 0:
            logger.warning(f"  âš ï¸ Invalid dimensions for image: {rect} (Page {page_num}) - Skipping")
            return output_path

        # ê³ í•´ìƒë„ ì´ë¯¸ì§€ ìƒì„±
        try:
            dpi_scale = dpi / 72
            mat = fitz.Matrix(dpi_scale, dpi_scale)
            pix = page.get_pixmap(matrix=mat, clip=rect)
            
            # PNGë¡œ ì €ì¥
            output_path.parent.mkdir(parents=True, exist_ok=True)
            pix.save(str(output_path))
        except Exception as e:
            logger.error(f"  âŒ Failed to save image {output_path}: {e}")
            return output_path
        
        # # [í›„ì²˜ë¦¬] PILë¡œ ìƒë‹¨ ê°•ì œ Crop (ì œëª© ì œê±°)
        # try:
        #     with Image.open(str(output_path)) as img:
        #         width, height = img.size
                
        #         # 120 DPI ê¸°ì¤€, ìƒë‹¨ 8px ì œê±° 
        #         # (150dpiì¼ ë•Œ 12px -> 120dpiì¼ ë•Œ ì•½ 9.6px -> 8~10px ì ì ˆ)
        #         # í—¤ë” ë³´ì¡´ì„ ìœ„í•´ ì¡°ê¸ˆ ë³´ìˆ˜ì ìœ¼ë¡œ 8px ì„¤ì • (crop=0ì€ ì‚¬ìš©ìê°€ ì§ì ‘ ì„¸íŒ…í–ˆì—ˆìœ¼ë¯€ë¡œ, ë¡œì§ì€ ìœ ì§€í•˜ë˜ ê°’ë§Œ ë³€ê²½)
        #         # User ìš”ì²­: crop=0 ì´ì—ˆì§€ë§Œ DPI ë°”ë€Œë©´ ë‹¤ì‹œ ì œëª© ë‚˜ì˜¬ ìˆ˜ ìˆìŒ.
        #         # í•˜ì§€ë§Œ UserëŠ” "ì§€ê¸ˆì€ ì œëª© ì•ˆë³´ì´ê³ ..." ë¼ê³  ë§Œì¡±í–ˆìœ¼ë¯€ë¡œ, DPI ë°”ë€Œë©´ ë¹„ìœ¨ ë§ì¶°ì•¼ í•¨.
        #         # ê·¸ëŸ¬ë‚˜ Userê°€ ë°©ê¸ˆ "í•´ìƒë„ë¥¼ ë³€ê²½í•˜ëŠ” ê²ƒì„ í•´ë³¼ê¹Œ?" í–ˆìœ¼ë¯€ë¡œ DPI ë³€ê²½ì— ì§‘ì¤‘.
        #         # crop_topì€ ì•ˆì „í•˜ê²Œ 0ìœ¼ë¡œ ë‘ê² ìŠµë‹ˆë‹¤. (Userê°€ 0ìœ¼ë¡œ ë§Œì¡±í–ˆìŒ)
        #         crop_top = 0
                
        #         # ì´ë¯¸ì§€ê°€ crop_top ë³´ë‹¤ ì¶©ë¶„íˆ í´ ë•Œë§Œ ìë¦„ (ìµœì†Œ 50px ë‚¨ê¹€)
        #         if height > crop_top + 50:
        #             cropped_img = img.crop((0, crop_top, width, height))
        #             cropped_img.save(str(output_path))
        #             # logger.info(f"  âœ‚ï¸ Cropped top {crop_top}px")
        # except Exception as e:
        #     logger.info(f"  âš ï¸ PIL Crop ì‹¤íŒ¨: {e}")
            
        return output_path
    
    def generate_figure_image(self, page_num: int, bbox: List[float], 
                             output_path: Path, 
                             margin_top: int = 1, margin_bottom: int = 1, 
                             margin_left: int = 1, margin_right: int = 1):
        """
        ê·¸ë¦¼ ì´ë¯¸ì§€ ìƒì„± (í…Œì´ë¸”ê³¼ ë™ì¼í•œ ë°©ì‹)
        
        Args:
            page_num: í˜ì´ì§€ ë²ˆí˜¸ (1-based)
            bbox: [x0, y0, x1, y1]
            output_path: ì¶œë ¥ íŒŒì¼ ê²½ë¡œ
            margin_*: ê° ë°©í–¥ë³„ ì—¬ë°± (í”½ì…€)
        """
        return self.generate_table_image(page_num, bbox, output_path, 
                                       margin_top=margin_top, margin_bottom=margin_bottom,
                                       margin_left=margin_left, margin_right=margin_right,
                                       dpi=TABLE_DPI)
    
    def process_section(self, section_file: Path, output_dir: Path):
        """
        ì„¹ì…˜ JSON íŒŒì¼ ì²˜ë¦¬
        """
        with open(section_file, 'r', encoding='utf-8') as f:
            section_data = json.load(f)
        
        section_id = section_data.get('section_id', '')
        section_title = section_data.get('title', '')
        
        def make_safe(s):
            return "".join([c if c.isalnum() else "_" for c in s]).strip("_")
            
        safe_id = make_safe(section_id)
        clean_title = section_title
        if section_id and section_title.startswith(section_id):
            clean_title = section_title[len(section_id):].strip()
        safe_title = make_safe(clean_title)
        
        while "__" in safe_id: safe_id = safe_id.replace("__", "_")
        while "__" in safe_title: safe_title = safe_title.replace("__", "_")

        BBOX_OVERRIDES = {
            "table_76_124": {"margin_bottom": 45},
        }
        
        json_modified = False
        tables = section_data['content']['tables']
        
        # --- Strict Grouping Logic based on User Rules ---
        # Rule 1: First has title, Next has no title -> Merge
        # Rule 2: Titles are same -> Merge
        # Rule 3: All have no titles -> Merge
        # Logic: Merge if (Current has No Title OR Current Title == Previous Title)
        
        grouped_tables = []
        if tables:
            current_group = [tables[0]]
            
            # Helper to check if title is "real" or missing/generic
            def get_real_title(t):
                # If title key is missing, or None, or starts with 'Table_' (our auto-gen), treat as None
                # Actually, step2 usually doesn't put 'title' key for generic ones.
                # But previous runs of step3 might have put 'Table_...'.
                # We should trust 'user-provided' titles (if any) or assume none.
                # Problem: How to distinguish 'Table_4.3...' from step3 vs real title?
                # Step 2 tables usually look like: {"id": "table_...", "bbox": ...} - No title.
                # So checking 'if "title" not in t' is safest for fresh run.
                # If re-running, step3 added titles.
                # We will check if it matches our auto-gen pattern.
                tit = t.get('title', '')
                if not tit: return None
                if tit.startswith('Table_' + safe_id): return None # Auto-generated
                return tit

            for i in range(1, len(tables)):
                curr_t = tables[i]
                prev_t = current_group[-1]
                
                curr_title = get_real_title(curr_t)
                prev_title = get_real_title(prev_t)
                
                # Merge condition:
                # 1. Current has NO title (Rule 1 & 3)
                # 2. OR Current title matches Previous title (Rule 2)
                should_merge = (curr_title is None) or (curr_title == prev_title)
                
                if should_merge:
                    current_group.append(curr_t)
                else:
                    grouped_tables.append(current_group)
                    current_group = [curr_t]
            
            grouped_tables.append(current_group)

        # --- Process Groups ---
        final_table_list = []
        
        for grp_idx, group in enumerate(grouped_tables):
            # Base name for this group
            # If multiple groups exist, we suffix _1, _2. If only 1 group, no suffix.
            group_suffix = ""
            if len(grouped_tables) > 1:
                group_suffix = f"_{grp_idx+1}"
            
            base_name = f"Table_{safe_id}_{safe_title}{group_suffix}"
            final_image_name = f"{base_name}.png"
            final_image_path = output_dir / final_image_name
            
            # 1. Generate individual images for stitching (or single image)
            temp_images = []
            for t_idx, table in enumerate(group):
                t_id = table.get('id', 'unknown')
                
                # We need a temp path
                temp_name = f"temp_{base_name}_part{t_idx}.png"
                temp_path = output_dir / temp_name
                
                margins = {
                    "margin_top": 2, "margin_bottom": 2, 
                    "margin_left": 2, "margin_right": 2
                }
                if t_id in BBOX_OVERRIDES:
                    margins.update(BBOX_OVERRIDES[t_id])
                
                self.generate_table_image(
                    page_num=table['page'],
                    bbox=table['bbox'],
                    output_path=temp_path,
                    dpi=TABLE_DPI,
                    **margins
                )
                temp_images.append(temp_path)
            
            # 2. Merge if needed (Group size > 1)
            if len(group) > 1:
                logger.info(f"  ğŸ”— Merging {len(group)} tables for {base_name}")
                
                images = [Image.open(p) for p in temp_images]
                total_height = sum(img.height for img in images)
                max_width = max(img.width for img in images)
                
                merged_img = Image.new('RGB', (max_width, total_height), (255, 255, 255))
                y = 0
                for img in images:
                    merged_img.paste(img, (0, y))
                    y += img.height
                
                merged_img.save(final_image_path)
                
            else:
                # Single table -> Just rename/move the temp file
                if temp_images[0].exists():
                    temp_images[0].replace(final_image_path)
            
            # Cleanup temps
            for p in temp_images:
                if p.exists() and p != final_image_path: p.unlink()
                
            # 3. Update JSON Entry
            # We take the first table of the group as the representative
            primary_table = group[0]
            primary_table['image_path'] = final_image_name
            primary_table['title'] = base_name
            
            # Metadata update
            if len(group) > 1:
                primary_table['merged_count'] = len(group)
                # Clear description if merging happened to force re-parse
                primary_table.pop('table_md', None)
            else:
                # If it was previously merged but now isn't (re-run scenario?), clear count
                primary_table.pop('merged_count', None)
                # Also clear table_md if image changed? 
                # Yes, safe to clear to ensure consistency.
                primary_table.pop('table_md', None)
            
            final_table_list.append(primary_table)
            json_modified = True

        # Update section data with the new list of (merged) tables
        section_data['content']['tables'] = final_table_list
        section_data['statistics']['table_count'] = len(final_table_list)

        # --- Figure Logic (Unchanged) ---
        figures = section_data['content']['figures']
        for i, figure in enumerate(figures):
            new_name_base = f"Figure_{safe_id}_{safe_title}"
            if len(figures) > 1:
                new_name_base += f"_{i+1}"
            
            image_name = f"{new_name_base}.png"
            output_path = output_dir / image_name
            
            if not output_path.exists():
                self.generate_figure_image(
                    page_num=figure['page'],
                    bbox=figure['bbox'],
                    output_path=output_path,
                    margin_top=0, margin_bottom=0, margin_left=0, margin_right=0
                )
                logger.info(f"  âœ“ ê·¸ë¦¼ ì´ë¯¸ì§€ ìƒì„±: {image_name}")

            figure['image_path'] = image_name
            if 'title' not in figure or not figure['title']:
                figure['title'] = new_name_base
            json_modified = True
        
        # Save
        if json_modified:
            with open(section_file, 'w', encoding='utf-8') as f:
                json.dump(section_data, f, indent=2, ensure_ascii=False)
        
        return len(final_table_list), len(figures)
    
    def process_all_sections(self, output_dir: str = "output/section_images"):
        """
        ëª¨ë“  ì„¹ì…˜ ì²˜ë¦¬
        
        Args:
            output_dir: ì´ë¯¸ì§€ ì¶œë ¥ ë””ë ‰í† ë¦¬
        """
        output_path = Path(output_dir)
        output_path.mkdir(parents=True, exist_ok=True)
        
        # JSON íŒŒì¼ ëª©ë¡
        json_files = sorted(self.section_data_dir.glob("*.json"))
        json_files = [f for f in json_files if f.name != "section_index.json"]
        
        logger.info(f"\nì´ {len(json_files)}ê°œ ì„¹ì…˜ ì²˜ë¦¬ ì‹œì‘...")
        logger.info(f"ì¶œë ¥ ë””ë ‰í† ë¦¬: {output_path}\n")
        
        total_tables = 0
        total_figures = 0
        
        for i, json_file in enumerate(json_files, 1):
            # ì„¹ì…˜ ì •ë³´ ì½ê¸°
            with open(json_file, 'r', encoding='utf-8') as f:
                section_data = json.load(f)
            
            # ì§„í–‰ ìƒí™© í‘œì‹œ (NameError ìˆ˜ì •ë¨)
            logger.info(f"[{i}/{len(json_files)}] {section_data.get('section_id', 'N/A')} - {section_data.get('title', 'Untitled')}")

            table_count = section_data['statistics']['table_count']
            figure_count = section_data['statistics']['figure_count']
            
            if table_count > 0 or figure_count > 0:
                logger.info(f"[{i}/{len(json_files)}] {section_data['section_id']} - {section_data['title']}")
                logger.info(f"  í…Œì´ë¸”: {table_count}ê°œ, ê·¸ë¦¼: {figure_count}ê°œ")
                
                t_count, f_count = self.process_section(json_file, output_path)
                total_tables += t_count
                total_figures += f_count
        
        logger.info(f"\nâœ… ì™„ë£Œ!")
        logger.info(f"ì´ í…Œì´ë¸” ì´ë¯¸ì§€: {total_tables}ê°œ")
        logger.info(f"ì´ ê·¸ë¦¼ ì´ë¯¸ì§€: {total_figures}ê°œ")
    
    def close(self):
        """ë¬¸ì„œ ë‹«ê¸°"""
        if self.doc:
            self.doc.close()


def main():
    """í…ŒìŠ¤íŠ¸ ì‹¤í–‰"""
    from common_parameter import PDF_PATH, OUTPUT_DIR
    
    logger.info("=" * 80)
    logger.info("Table/Figure Image Generator - í…Œì´ë¸”/ê·¸ë¦¼ ì´ë¯¸ì§€ ìƒì„±")
    logger.info("=" * 80)
    
    # ë””ë ‰í† ë¦¬ í™•ì¸
    section_data_dir = Path(OUTPUT_DIR) / "section_data_v2"
    image_dir = Path(OUTPUT_DIR) / "section_images"
    image_dir.mkdir(parents=True, exist_ok=True)
    
    generator = TableImageGenerator(
        pdf_path=PDF_PATH,
        section_data_dir=section_data_dir
    )
    
    try:
        generator.process_all_sections(
            output_dir=image_dir
        )
    finally:
        generator.close()


if __name__ == '__main__':
    main()
